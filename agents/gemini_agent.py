"""Gemini ê¸°ë°˜ ê¸°ìˆ  ì •í™•ì„± ì „ë¬¸ê°€ Agent"""

import google.generativeai as genai
from typing import Dict, List, Optional, Any
from agents.base_agent import BaseAgent, AgentConfig, AgentResponse, AgentError
from config.settings import LLM_CONFIGS
import logging

logger = logging.getLogger(__name__)

class GeminiAgent(BaseAgent):
    """Gemini ê¸°ë°˜ ê¸°ìˆ ì  ì •í™•ì„± ë° ìˆ˜ì¹˜ ë¶„ì„ ì „ë¬¸ê°€"""

    def __init__(self):
        config = AgentConfig(
            name="Gemini",
            specialty="ê¸°ìˆ ì  ì •í™•ì„± ë° ìˆ˜ì¹˜ ë¶„ì„",
            model=LLM_CONFIGS["google"]["model"],
            max_tokens=LLM_CONFIGS["google"]["max_tokens"],
            temperature=LLM_CONFIGS["google"]["temperature"]
        )
        super().__init__(config)

        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        genai.configure(api_key=LLM_CONFIGS["google"]["api_key"])
        self.model_instance = genai.GenerativeModel(self.model)

    async def analyze_and_respond(self, state: Dict[str, Any]) -> AgentResponse:
        """Gemini ê¸°ë°˜ ê¸°ìˆ  ë¶„ì„"""

        self.validate_input(state)

        user_question = state.get('user_message', '')
        rag_context = state.get('rag_context', {})
        issue_classification = state.get('issue_classification', {})
        conversation_history = state.get('conversation_history', [])
        
        print(f"ğŸ” Gemini Agent - conversation_history ìˆ˜: {len(conversation_history)}")
        if conversation_history:
            print(f"ğŸ” Gemini Agent - ì²« ë²ˆì§¸ ëŒ€í™”: {conversation_history[0]}")
        else:
            print(f"ğŸ” Gemini Agent - conversation_historyê°€ ë¹„ì–´ìˆìŒ")
        
        # ë™ì  í† í° í•œê³„ ê³„ì‚°  
        from utils.token_manager import get_token_manager
        token_manager = get_token_manager()
        dynamic_max_tokens = token_manager.get_agent_specific_limit('gemini', state)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.build_technical_prompt(user_question, rag_context or {}, issue_classification or {}, conversation_history)

        try:
            logger.info(f"Gemini Agent ë¶„ì„ ì‹œì‘ - ëª¨ë¸: {self.model}, í† í° í•œê³„: {dynamic_max_tokens}")

            # Gemini API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = await self._generate_content_async(prompt)
                    break
                except Exception as e:
                    if "overloaded" in str(e) and attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„: 1s, 2s, 4s
                        logger.warning(f"Gemini ê³¼ë¶€í•˜, {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        import asyncio
                        await asyncio.sleep(wait_time)
                        continue
                    raise e

            response_text = response.text
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (GeminiëŠ” ì •í™•í•œ í† í° ìˆ˜ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ)
            estimated_tokens = len(response_text) // 4  # ëŒ€ëµì ì¸ ì¶”ì •
            token_usage = {
                "estimated_total_tokens": estimated_tokens,
                "estimated_completion_tokens": estimated_tokens // 2
            }
            
            confidence = self.calculate_confidence(len(response_text), token_usage)

            logger.info(f"Gemini Agent ë¶„ì„ ì™„ë£Œ - ì˜ˆìƒ í† í°: {estimated_tokens}")

            return self.create_response(
                response_text=response_text,
                confidence=confidence,
                processing_time=0.0,
                token_usage=token_usage
            )

        except Exception as e:
            logger.error(f"Gemini Agent ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            raise AgentError(f"ê¸°ìˆ  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", self.name, "ANALYSIS_ERROR")

    async def _generate_content_async(self, prompt: str):
        """ë¹„ë™ê¸° ì»¨í…ì¸  ìƒì„±"""
        import asyncio

        # GeminiëŠ” ë¹„ë™ê¸°ë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ executor ì‚¬ìš©
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.model_instance.generate_content, prompt)

    def build_technical_prompt(self, question: str, rag_context: Dict, issue_info: Dict, conversation_history: Optional[List] = None) -> str:
        """ê¸°ìˆ ì  ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        # ê¸°ìˆ  ë°ì´í„° ì¶”ì¶œ
        technical_data = ""
        if rag_context.get('chroma_results'):
            technical_data += "ê¸°ìˆ  ì‚¬ì–‘ ë° ë°ì´í„°:\n"
            for i, result in enumerate(rag_context['chroma_results'][:3], 1):
                # RAGResult ê°ì²´ì™€ dictionary ë‘˜ ë‹¤ ì²˜ë¦¬
                if hasattr(result, 'content'):
                    content = result.content[:250]
                else:
                    content = result.get('content', '')[:250]
                if any(keyword in content.lower() for keyword in ['ì••ë ¥', 'ì˜¨ë„', 'ì „ë¥˜', 'ì „ì••', 'ì§„ë™', 'ë‘ê»˜']):
                    technical_data += f"{i}. {content}...\n"

        # ëŒ€í™” ê¸°ë¡ ì •ë¦¬
        conversation_context = ""
        if conversation_history:
            conversation_context = "\nì´ì „ ê¸°ìˆ  ìƒë‹´ ê¸°ë¡:\n"
            for i, conv in enumerate(conversation_history[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
                if isinstance(conv, dict):
                    # ë©”ì‹œì§€ í˜•ì‹ê³¼ ê¸°ì¡´ í˜•ì‹ ëª¨ë‘ ì§€ì›
                    if conv.get('role') == 'user':
                        user_msg = conv.get('content', '')
                        conversation_context += f"{i}. ì‚¬ìš©ì: {user_msg}\n"
                    elif conv.get('role') == 'assistant':
                        bot_msg = conv.get('content', '')
                        conversation_context += f"   â†’ ì´ì „ ë‹µë³€: {bot_msg[:100]}...\n"
                    else:
                        # ê¸°ì¡´ í˜•ì‹ ì§€ì›
                        user_msg = conv.get('user_message', '')
                        bot_response = conv.get('bot_response', '')
                        timestamp = conv.get('timestamp', '')
                        agents_used = conv.get('agents_used', [])
                        if user_msg:
                            conversation_context += f"{i}. [{timestamp[:16]}] ê¸°ìˆ  ë¬¸ì˜: {user_msg}\n"
                            if bot_response:
                                conversation_context += f"   â†’ ì´ì „ ë‹µë³€: {bot_response[:200]}...\n"
                            if agents_used:
                                conversation_context += f"   â†’ ë¶„ì„ ì „ë¬¸ê°€: {', '.join(agents_used)}\n"

        # ì´ìŠˆ ê¸°ìˆ  ì •ë³´
        technical_context = ""
        if issue_info.get('issue_info') and not issue_info['issue_info'].get('error'):
            issue_data = issue_info['issue_info']
            technical_context = f"""
ê¸°ìˆ ì  ë°°ê²½:
- ë¬¸ì œ ìœ í˜•: {issue_data.get('description', '')}
- ê¸°ìˆ  ì¹´í…Œê³ ë¦¬: {issue_data.get('category', '')}
- ê´€ë ¨ ë¶€í’ˆ: {', '.join(issue_data.get('affected_components', []))}
"""

        return f"""
ì‚¬ìš©ì ê¸°ìˆ  ë¬¸ì˜: {question}

{conversation_context}

{technical_context}

ê¸°ìˆ  ìë£Œ:
{technical_data}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ  ì „ë¬¸ê°€ ê´€ì ì—ì„œ ë‹¤ìŒì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì´ì „ ê¸°ìˆ  ìƒë‹´ì´ ìˆë‹¤ë©´ ê·¸ ì—°ì†ì„±ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. ì •í™•í•œ ê¸°ìˆ ì  ì›ì¸ ë¶„ì„
2. ìˆ˜ì¹˜ ë° ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼
3. ê³µí•™ì  ê³„ì‚° ë° ê²€ì¦
4. ê¸°ìˆ  í‘œì¤€ ë° ê·œê²© ì¤€ìˆ˜
5. ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ
6. ì •ë°€í•œ ì¸¡ì • ë° ê²€ì‚¬ ë°©ë²•

ëª¨ë“  ì œì•ˆì€ ê¸°ìˆ ì  ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œí•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì‚¬ì–‘ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
"""

    def get_strengths(self) -> List[str]:
        """Gemini Agentì˜ ê°•ì """
        return ["ê¸°ìˆ ì •í™•ì„±", "ìˆ˜ì¹˜ë¶„ì„", "ê³µí•™ê³„ì‚°", "ì„±ëŠ¥ìµœì í™”", "ì •ë°€ì¸¡ì •"]

    def get_focus_areas(self) -> List[str]:
        """Gemini Agentì˜ ì¤‘ì  ì˜ì—­"""
        return ["ê¸°ìˆ ë¶„ì„", "ë°ì´í„°ê²€ì¦", "ì„±ëŠ¥ì¸¡ì •", "ê·œê²©ì¤€ìˆ˜", "ìµœì í™”"]

    def calculate_confidence(self, response_length: int, token_usage: Optional[Dict[str, int]] = None) -> float:
        """Gemini ì‘ë‹µ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.75  # Gemini ê¸°ë³¸ ì‹ ë¢°ë„

        # ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        if response_length > 600:
            base_confidence += 0.15
        elif response_length < 150:
            base_confidence -= 0.15

        # ê¸°ìˆ ì  í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ë¡œ ì¶”ê°€ ê°€ì¤‘ì¹˜
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” response_textë¥¼ ë¶„ì„)
        base_confidence += 0.05  # ê¸°ë³¸ ê¸°ìˆ  ê°€ì¤‘ì¹˜

        return min(0.95, max(0.3, base_confidence))